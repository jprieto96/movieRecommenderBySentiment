{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import joblib\n",
    "from wordcloud import WordCloud\n",
    "from tweepy import OAuthHandler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "source": [
    "### Cargamos el modelo Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('lr_model.joblib')"
   ]
  },
  {
   "source": [
    "### Se cargan las claves para acceder a la API de Twitter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# keys and tokens from the Twitter Dev Console\n",
    "consumer_key = 'rKyttCwUrUgz6JFyJMGPzLWWi'\n",
    "consumer_secret = 'b7k9Mi0iNkFHJtEd3KDFSgb86NdhI47uIxCTKA687F0TXh4n6r'\n",
    "access_token = '1394615213199081475-xDnRhKd4El8p9MjjqGr7uYJ32ckx6q'\n",
    "access_token_secret = 'YpPRz14y3KcpnwK7tE67wMWWCFKC5lJyp62ZIYN8r3Voh'\n",
    "\n",
    "username = \"MinPrac\"\n",
    "\n",
    "# attempt authentication\n",
    "try:\n",
    "    # create OAuthHandler object\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    # set access token and secret\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # create tweepy API object to fetch tweets\n",
    "    api = tweepy.API(auth)\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")"
   ]
  },
  {
   "source": [
    "### Se cargan los ultimos 10 tweets del usuario indicado y se muestran los 5 mas recientes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Show the 5 recent tweets:\n\n1) I was sad\n\n2) Keep up the good work!\n\n3) I am very happy with the news\n\n4) Today has been a great day for all the retail investors\n\n5) I need a break\n\n"
     ]
    }
   ],
   "source": [
    "# Extract 10 tweets from the twitter user\n",
    "posts = api.user_timeline(screen_name=username, count = 10, lang =\"en\", tweet_mode=\"extended\")\n",
    "\n",
    "#  Print the last 5 tweets\n",
    "print(\"Show the 5 recent tweets:\\n\")\n",
    "i=1\n",
    "for tweet in posts[:5]:\n",
    "    print(str(i) +') '+ tweet.full_text + '\\n')\n",
    "    i= i+1"
   ]
  },
  {
   "source": [
    "### Creamos el dataframe con los tweets cargados en el paso anterior"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets\n",
       "0                                          I was sad\n",
       "1                             Keep up the good work!\n",
       "2                      I am very happy with the news\n",
       "3  Today has been a great day for all the retail ...\n",
       "4                                     I need a break"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I was sad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Keep up the good work!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am very happy with the news</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Today has been a great day for all the retail ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I need a break</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Create a dataframe with a column called Tweets\n",
    "df = pd.DataFrame([tweet.full_text for tweet in posts], columns=['Tweets'])\n",
    "# Show the first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Limpieza del texto\n",
    "Hacemos una limpia del texto del tweet. Se eliminan las URLs, todas las referencias, los hashtags, las puntuaciones, eliminar las stopwords y por ultimo usamos la técnica de Lematización.\n",
    "<br/><br/>\n",
    "La lematización es un proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), hallar el lema correspondiente.\n",
    "<br/><br/>\n",
    "![title](lematizacion.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            Tweets\n",
       "0                            I sad\n",
       "1                   keep good work\n",
       "2                     I happi news\n",
       "3  today great day retail investor\n",
       "4                     I need break\n",
       "5                         I hungri\n",
       "6                      feel devast"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I sad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>keep good work</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I happi news</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>today great day retail investor</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I need break</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I hungri</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>feel devast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "df['Tweets'] = df['Tweets'].apply(preprocess_tweet_text)\n",
    "df"
   ]
  },
  {
   "source": [
    "### Usamos TFidf Vectorizer\n",
    "TF-IDF (Frecuencia de documento de frecuencia de inversión de término) es una técnica de ponderación comúnmente utilizada en el procesamiento de información y la minería de datos. Esta técnica utiliza un método estadístico para calcular la importancia de una palabra en todo el corpus en función del número de veces que la palabra aparece en el texto y la frecuencia de los documentos que aparecen en todo el corpus. Su ventaja es que puede filtrar algunas palabras comunes pero irrelevantes, mientras retiene palabras importantes que afectan todo el texto.\n",
    "<br/></br>\n",
    "TF (Frecuencia de término) indica con qué frecuencia aparece una palabra clave en todo el artículo.\n",
    "<br/>\n",
    "IDF (Frecuencia de documento de inversión) significa calcular la frecuencia de texto invertida. La frecuencia del texto se refiere al número de veces que aparece una palabra clave en todos los artículos de todo el corpus. La frecuencia de documento invertida también se denomina frecuencia de documento inversa. Es el recíproco de la frecuencia de documento. Se utiliza principalmente para reducir el efecto de algunas palabras comunes en todos los documentos que tienen poco efecto en el documento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = joblib.load('tf_model.joblib')\n",
    "    vector.transform(train_fit)\n",
    "    return vector\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(df.iloc[:]).ravel())\n",
    "X = tf_vector.transform(np.array(df.iloc[:]).ravel())"
   ]
  },
  {
   "source": [
    "### Predicción del modelo cargado anteriormente (Logistic Regression)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            Tweets Sentiment\n",
       "0                            I sad  negative\n",
       "1                   keep good work  positive\n",
       "2                     I happi news  positive\n",
       "3  today great day retail investor  positive\n",
       "4                     I need break  negative\n",
       "5                         I hungri  negative\n",
       "6                      feel devast  negative"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I sad</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>keep good work</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I happi news</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>today great day retail investor</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I need break</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I hungri</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>feel devast</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "predicted_sentiments = loaded_model.predict(X)\n",
    "df['Sentiment'] = ['negative' if sentiment == 0 else 'positive' for sentiment in predicted_sentiments]\n",
    "df"
   ]
  },
  {
   "source": [
    "### Preparación del CSV para luego cargarlo en el recomendador de peliculas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing CSV files\n",
    "\n",
    "user = api.get_user(username)\n",
    "user_id = user.id_str\n",
    "\n",
    "positiveTweets = 0\n",
    "negativeTweets = 0\n",
    "for sentiment in df['Sentiment']:\n",
    "    if sentiment == 'positive':\n",
    "        ++positiveTweets\n",
    "    else:\n",
    "        ++negativeTweets\n",
    "\n",
    "status = 'positive' if positiveTweets > negativeTweets else 'negative'\n",
    "\n",
    "diccionario = {'user_id': [user_id], 'user_name': [username], 'status': [status]}\n",
    "\n",
    "df_user = pd.DataFrame(diccionario)\n",
    "\n",
    "df_user.to_csv('userSentiment.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0fec86042567a2cc61725b797b4a1d1b1931e2d53dc8382f7c69e48cc2183c59d",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}