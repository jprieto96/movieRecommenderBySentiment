{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import joblib\n",
    "from wordcloud import WordCloud\n",
    "from tweepy import OAuthHandler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "source": [
    "### Cargamos el modelo Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('lr_model.joblib')"
   ]
  },
  {
   "source": [
    "### Se cargan las claves para acceder a la API de Twitter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# keys and tokens from the Twitter Dev Console\n",
    "consumer_key = 'rKyttCwUrUgz6JFyJMGPzLWWi'\n",
    "consumer_secret = 'b7k9Mi0iNkFHJtEd3KDFSgb86NdhI47uIxCTKA687F0TXh4n6r'\n",
    "access_token = '1394615213199081475-xDnRhKd4El8p9MjjqGr7uYJ32ckx6q'\n",
    "access_token_secret = 'YpPRz14y3KcpnwK7tE67wMWWCFKC5lJyp62ZIYN8r3Voh'\n",
    "\n",
    "username = \"MinPrac\"\n",
    "\n",
    "# attempt authentication\n",
    "try:\n",
    "    # create OAuthHandler object\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    # set access token and secret\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # create tweepy API object to fetch tweets\n",
    "    api = tweepy.API(auth)\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")"
   ]
  },
  {
   "source": [
    "### Se cargan los ultimos 10 tweets del usuario indicado y se muestran los 5 mas recientes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Show the 5 recent tweets:\n\n1) I’m answering your questions now on \n@reddit\n: https://t.co/7pTurjiOCG\n\n2) I wanna kill myself\n\n3) Make humanity a multiplanet species!\n\n4) Being happy never goes out of style\n\n5) I loveee Madrid\n\n"
     ]
    }
   ],
   "source": [
    "# Extract 10 tweets from the twitter user\n",
    "posts = api.user_timeline(screen_name=username, count = 10, lang =\"en\", tweet_mode=\"extended\")\n",
    "\n",
    "#  Print the last 5 tweets\n",
    "print(\"Show the 5 recent tweets:\\n\")\n",
    "i=1\n",
    "for tweet in posts[:5]:\n",
    "    print(str(i) +') '+ tweet.full_text + '\\n')\n",
    "    i= i+1"
   ]
  },
  {
   "source": [
    "### Creamos el dataframe con los tweets cargados en el paso anterior"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets\n",
       "0  I’m answering your questions now on \\n@reddit\\...\n",
       "1                                I wanna kill myself\n",
       "2               Make humanity a multiplanet species!\n",
       "3                Being happy never goes out of style\n",
       "4                                    I loveee Madrid"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I’m answering your questions now on \\n@reddit\\...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I wanna kill myself</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Make humanity a multiplanet species!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Being happy never goes out of style</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I loveee Madrid</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Create a dataframe with a column called Tweets\n",
    "df = pd.DataFrame([tweet.full_text for tweet in posts], columns=['Tweets'])\n",
    "# Show the first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Limpieza del texto\n",
    "Hacemos una limpia del texto del tweet. Se eliminan las URLs, todas las referencias, los hashtags, las puntuaciones, eliminar las stopwords y por ultimo usamos la técnica de Lematización.\n",
    "<br/><br/>\n",
    "La lematización es un proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), hallar el lema correspondiente.\n",
    "<br/><br/>\n",
    "![title](lematizacion.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            Tweets\n",
       "0              I ’ answer question\n",
       "1                    I wan na kill\n",
       "2     make human multiplanet speci\n",
       "3         be happi never goe style\n",
       "4                   I lovee madrid\n",
       "5                            I sad\n",
       "6                   keep good work\n",
       "7                     I happi news\n",
       "8  today great day retail investor\n",
       "9                     I need break"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I ’ answer question</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I wan na kill</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>make human multiplanet speci</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>be happi never goe style</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I lovee madrid</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I sad</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>keep good work</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I happi news</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>today great day retail investor</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>I need break</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "df['Tweets'] = df['Tweets'].apply(preprocess_tweet_text)\n",
    "df"
   ]
  },
  {
   "source": [
    "### Usamos TFidf Vectorizer\n",
    "TF-IDF (Frecuencia de documento de frecuencia de inversión de término) es una técnica de ponderación comúnmente utilizada en el procesamiento de información y la minería de datos. Esta técnica utiliza un método estadístico para calcular la importancia de una palabra en todo el corpus en función del número de veces que la palabra aparece en el texto y la frecuencia de los documentos que aparecen en todo el corpus. Su ventaja es que puede filtrar algunas palabras comunes pero irrelevantes, mientras retiene palabras importantes que afectan todo el texto.\n",
    "<br/></br>\n",
    "TF (Frecuencia de término) indica con qué frecuencia aparece una palabra clave en todo el artículo.\n",
    "<br/>\n",
    "IDF (Frecuencia de documento de inversión) significa calcular la frecuencia de texto invertida. La frecuencia del texto se refiere al número de veces que aparece una palabra clave en todos los artículos de todo el corpus. La frecuencia de documento invertida también se denomina frecuencia de documento inversa. Es el recíproco de la frecuencia de documento. Se utiliza principalmente para reducir el efecto de algunas palabras comunes en todos los documentos que tienen poco efecto en el documento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = joblib.load('tf_model.joblib')\n",
    "    vector.transform(train_fit)\n",
    "    return vector\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(df.iloc[:]).ravel())\n",
    "X = tf_vector.transform(np.array(df.iloc[:]).ravel())"
   ]
  },
  {
   "source": [
    "### Predicción del modelo cargado anteriormente (Logistic Regression)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            Tweets Sentiment\n",
       "0              I ’ answer question  positive\n",
       "1                    I wan na kill  negative\n",
       "2     make human multiplanet speci  positive\n",
       "3         be happi never goe style  positive\n",
       "4                   I lovee madrid  positive\n",
       "5                            I sad  negative\n",
       "6                   keep good work  positive\n",
       "7                     I happi news  positive\n",
       "8  today great day retail investor  positive\n",
       "9                     I need break  negative"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I ’ answer question</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I wan na kill</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>make human multiplanet speci</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>be happi never goe style</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I lovee madrid</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I sad</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>keep good work</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I happi news</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>today great day retail investor</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>I need break</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "predicted_sentiments = loaded_model.predict(X)\n",
    "df['Sentiment'] = ['negative' if sentiment == 0 else 'positive' for sentiment in predicted_sentiments]\n",
    "df"
   ]
  },
  {
   "source": [
    "### Preparación del CSV para luego cargarlo en el recomendador de peliculas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing CSV files\n",
    "\n",
    "user = api.get_user(username)\n",
    "user_id = user.id_str\n",
    "\n",
    "positiveTweets = 0\n",
    "negativeTweets = 0\n",
    "for sentiment in df['Sentiment']:\n",
    "    if sentiment == 'positive':\n",
    "        positiveTweets+=1\n",
    "    else:\n",
    "        negativeTweets+=1\n",
    "\n",
    "status = 'positive' if positiveTweets > negativeTweets else 'negative'\n",
    "\n",
    "diccionario = {'user_id': [user_id], 'user_name': [username], 'status': [status]}\n",
    "\n",
    "df_user = pd.DataFrame(diccionario)\n",
    "\n",
    "df_user.to_csv('userSentiment.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0fec86042567a2cc61725b797b4a1d1b1931e2d53dc8382f7c69e48cc2183c59d",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}